# Investigation-of-gradient-descent-optimization-methods

Работа, проведённая мной на летней практике 1 курса обучения.


Методы градиентного спуска лежат в основе обучения искусственных нейронных сетей, позволяя минимизировать функцию потерь и находить оптимальные параметры модели. Я исследовала два таких метода: SGD и Adam, а также ансамблевый метод градиентного бустинга AdaBoost. Методы были сравнены в задаче распознавания языка текста, а также по поиску глобального минимума у функций Растригина, Экли и Розенброка.

